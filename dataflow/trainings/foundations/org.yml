beam portability:

Apache Beam:
  vision:
    - provide a comprehensive portability framework for data processing pipelines
    - this fw would allow write pipeline in our favourite language:
        - Java:
            - input.apply(Sum.IntergersPerKey())
        - Python:
            - input | Sum.PerKey()
        - Go:
            -  Stats.sum(s,input)
        - SQL:
            - SELECT key, SUM(value) FROM input GROUP BY key
    - then would run it in the execution engine of choice:
        -  Dataflow
        - Apache Flink
        - Spark
        - Apex
        - IBM Streams
        - Apache Samza
        -
    - like if we decide what engine we want our car to run every time we drive
  programming model:
    - define batch data proc
    - define streaming data proc
  Beam SDK:
    - to create a pipeline in the language we like
  Pipeline:
    Runner: Used to execute the pipeline, Dataflow is an example!
      - can run locally
      - in datacenter
      - using services of cloud provider
  Dataflow:
    - fully managed data processing service
    - automated provisioning and managemente of processing resources
    - uses horizontal services like Logging and Monitoring
  Portability Framework:
    - language agnostic for presentation
    - protocol for execution
    - Portability API is called this interoperability layer
    - Every runner can work with every supported language
    - Containarization allows a hermetic working env:
        -  MultiLanguage pipelines
        - cross-language transform
  RunnerV2: improved
  Container environments:
    - Beam SDK runtime env can be conteinerized with Docker isolated from other runtime systems
    - each operation separated env
    - custom container with Apache Beam version greater than 2.25.o
    - to test locally we need docker installed
    - we start with a dockerfile starting with beam
    - build and put in registry:
        - gcloud builds submit -t $IMAGE_URI
        - docker build -f Dockerfile & docker push
    - launch Dataflow job
  Cross-Language-Transform:
    - kinf of you can use a different language for every step, like if every step is abstracted away by the framework
    - Interesting let's say a connector is only available in Java, you do that in Java and all the rest in Python taht you like more
Separate Compute and Storage:
  Shufle Service: only for batch
  Dataflow Streaming Engine:
  Flex RS job: for workloads that can be delayed
IAM:
  Beam code we want to run in GC:
    submit pipeline:
      - code to Cloudstorage
      - three credentials needed:
          three user role:
            Dataflow viewer: just to see
            Dataflow Developer: submit, stqage files to cloud storage,view the available CE quota
            Dataflow admin:
          Dataflow service account:
          Controller Service Account:
            - created when enabled CE API
      For prod a svc account with minimal permisioons
Quotas:
  CPU: wrror if exceeded
  Ips:
  Persistent Disks:
    - For Batch
    - For Streaming:
        amound disk allocated == max number workersn (maximum is 1000)
Sec:
  Data Locality:
    Data and Metadata stays i 1 region
  Shared VPC:
    Dataflow jobs can run on VPC or shared VPC
  Disable external IPs: by default DF service assings external and internal
  CMEK:
    where data is stored
    data keys in grouping operations (key based, windowing, grouping, joining) are decryoted using CMEK key, for add sec we can hash or transform the key
    job Metadata is protected with G encryot, job name, job param values, system generated data such as IPs

  Lab: Setup IAM and Networking for your Dataflow Jobs
    what: In this lab, you will learn to set up IAM permissions and use private IP adresses for your Datafow jobs.
    Activate Cloud Shell:
      - Cloud Shell is a virtual machine that contains development tools.
      - It offers a persistent 5-GB home directory and runs on Google Cloud
      - gcloud auth list: List the active account name
      - gcloud config list project: List the project ID
      - tasks:
          Task 1. Create a Cloud Storage bucket:
            PROJECT=`gcloud config list --format 'value(core.project)'`
            USER_EMAIL=`gcloud config list account --format "value(core.account)"`
            REGION=us-central1
            gsutil mb -p $PROJECT -b on gs://$PROJECT
          Task 2. Launch a Dataflow job:

            gcloud projects get-iam-policy $PROJECT  \
            --format='table(bindings.role)' \
            --flatten="bindings[].members" \
            --filter="bindings.members:$USER_EMAIL"

            gcloud dataflow jobs run job1 \
            --gcs-location gs://dataflow-templates-us-central1/latest/Word_Count \
            --region $REGION \
            --staging-location gs://$PROJECT/tmp \
            --parameters inputFile=gs://dataflow-samples/shakespeare/kinglear.txt,output=gs://$PROJECT/results/outputs

            fail: This will fail as expected because of missing IAM permissions.
            Add the Dataflow Admin role to the user account:
            gcloud projects add-iam-policy-binding $PROJECT --member=user:$USER_EMAIL --role=roles/dataflow.admin
            
            gcloud dataflow jobs run job1 \
            --gcs-location gs://dataflow-templates-us-central1/latest/Word_Count \
            --region $REGION \
            --staging-location gs://$PROJECT/tmp \
            --parameters inputFile=gs://dataflow-samples/shakespeare/kinglear.txt,output=gs://$PROJECT/results/outputs
          Task 3. Launch in private IPs:
            In this task, you first try to launch a Dataflow job with the --disable-public-ips directive. It will fail in the first attempt because the network does not have Private Google Access (PGA) turned on. You configure PGA and re-run the command to launch the job.

            gcloud dataflow jobs run job2 \
            --gcs-location gs://dataflow-templates-us-central1/latest/Word_Count \
            --region $REGION \
            --staging-location gs://$PROJECT/tmp \
            --parameters inputFile=gs://dataflow-samples/shakespeare/kinglear.txt,output=gs://$PROJECT/results/outputs --disable-public-ips

            To verify, go to the Google Cloud Console, on the Navigation menu, click Dataflow > Jobs, and notice that job2 failed. Click on job2, then scroll to the bottom to click on "SHOW" next to Logs to see the cause of error.

            gcloud projects add-iam-policy-binding $PROJECT --member=user:$USER_EMAIL --role=roles/compute.networkAdmin
            gcloud compute networks subnets update default \
            --region=$REGION \
            --enable-private-ip-google-access
            
            gcloud dataflow jobs run job2 \
            --gcs-location gs://dataflow-templates-us-central1/latest/Word_Count \
            --region $REGION \
            --staging-location gs://$PROJECT/tmp \
            --parameters inputFile=gs://dataflow-samples/shakespeare/kinglear.txt,output=gs://$PROJECT/results/outputs --disable-public-ips
            
            In the Google Cloud Console, on the Navigation menu, click Compute Engine > VM Instances, and 
            notice that the VM launched has no external IP address!!